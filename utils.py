import os
from pathlib import Path
from typing import Optional, Union, Dict, Any
from langchain.agents import initialize_agent, AgentType
from langchain_community.utilities.serpapi import SerpAPIWrapper
from langchain.tools import Tool
from langchain.chains import ConversationChain, ConversationalRetrievalChain
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from memory import MemoryManager


def online_search_agent(openai_key: str,
                        serp_key: Optional[str] = None,
                        model_name: str = "gpt-3.5-turbo",
                        api_base: str = "https://api.aigc369.com/v1"):
    """åŸºäº SerpAPI çš„è”ç½‘æœç´¢ Agent"""
    if not serp_key:
        serp_key = os.getenv("SERPAPI_API_KEY")
        if not serp_key:
            raise ValueError("éœ€è¦æä¾› SERPAPI_API_KEY")
    os.environ["SERPAPI_API_KEY"] = serp_key

    search = SerpAPIWrapper()

    def search_wrapper(query: str) -> str:
        result = search.run(query)
        # ä¿è¯è¾“å‡ºä¸ºçº¯æ–‡æœ¬æ ¼å¼
        if isinstance(result, list):
            return "\n".join(result)
        return str(result)

    tool = Tool(
        name="Search",
        func=search_wrapper,
        description="ç”¨äºæ£€ç´¢å½“å‰äº‹ä»¶ä¿¡æ¯"
    )

    llm = ChatOpenAI(
        model=model_name,
        openai_api_key=openai_key,
        openai_api_base=api_base
    )
    return initialize_agent(
        tools=[tool],
        llm=llm,
        agent=AgentType.OPENAI_FUNCTIONS,
        verbose=True
    )


def get_chat_response(prompt: str,
                      memory_manager: Optional[MemoryManager] = None,
                      openai_api_key: str = None,
                      model_name: str = "gpt-3.5-turbo",
                      use_docs: bool = False,
                      embed_dir: Optional[Union[str, Path]] = None,
                      chain_of_thought: bool = False,
                      api_base: str = "https://api.aigc369.com/v1"):
    """
    è·å–èŠå¤©å“åº”ï¼Œæ”¯æŒçŸ­æœŸå’Œé•¿æœŸè®°å¿†ï¼Œå¢å¼ºå¥å£®æ€§é˜²æ­¢ answer æŠ¥é”™
    """
    import streamlit as st
    import json
    from langchain.chains import ConversationChain, ConversationalRetrievalChain
    from langchain_chroma import Chroma
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_openai import ChatOpenAI

    # åˆå§‹åŒ– LLM
    llm = ChatOpenAI(
        model=model_name,
        temperature=0.2,
        openai_api_key=openai_api_key,
        openai_api_base=api_base
    )

    # åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨
    if memory_manager is None:
        if "memory_manager" in st.session_state:
            memory_manager = st.session_state.memory_manager
        else:
            memory_manager = MemoryManager(llm)
            st.session_state.memory_manager = memory_manager

    # å¦‚æœå¯ç”¨æ–‡æ¡£æ£€ç´¢
    if use_docs and embed_dir and Path(embed_dir).exists():
        vectordb = Chroma(
            persist_directory=str(embed_dir),
            embedding_function=HuggingFaceEmbeddings(
                model_name="./models/all-MiniLM-L6-v2",
                model_kwargs={"device": "cpu"}
            )
        )

        # è·å–ç›¸å…³å†å²è®°å½•
        relevant_memory = memory_manager.get_relevant_history(prompt)
        messages = relevant_memory["short_term"].get("chat_history", [])

        # åˆ›å»ºæ£€ç´¢é“¾
        chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectordb.as_retriever(),
            return_source_documents=True,
            verbose=chain_of_thought,
            output_key="answer"
        )

        try:
            # å°†æ¶ˆæ¯å†å²è½¬æ¢ä¸ºå…ƒç»„åˆ—è¡¨æ ¼å¼
            chat_history = []
            for i in range(0, len(messages), 2):
                if i + 1 < len(messages):
                    human_msg = messages[i]
                    ai_msg = messages[i + 1]
                    chat_history.append((human_msg.content, ai_msg.content))

            # è°ƒç”¨æ£€ç´¢é“¾
            result = chain.invoke({
                "question": prompt,
                "chat_history": chat_history
            })

            # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
            st.code(json.dumps(result, indent=2, ensure_ascii=False), language="json")
            print("[DEBUG] Result type:", type(result))
            print("[DEBUG] Result content:", result)

            if isinstance(result, dict):
                answer = result.get("answer") or result.get("output") or "ğŸ¤– æ²¡æœ‰è·å–åˆ°å›ç­”å†…å®¹ã€‚"
            else:
                answer = str(result)

        except Exception as e:
            answer = f"âš ï¸ é“¾è°ƒç”¨å¼‚å¸¸ï¼š{str(e)}"

        memory_manager.add_interaction(prompt, answer)

        st.session_state.chat_history = st.session_state.get("chat_history", [])
        st.session_state.chat_history.append((prompt, answer))

        return answer

    # å¦åˆ™ä¸ºæ™®é€šå¯¹è¯æ¨¡å¼
    try:
        # è·å–å†å²æ¶ˆæ¯
        messages = memory_manager.short_term.message_history.messages
        
        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        context = "\n".join([
            f"Human: {msg.content}" if msg.type == "human" else f"Assistant: {msg.content}"
            for msg in messages[-6:]  # åªä½¿ç”¨æœ€è¿‘çš„3è½®å¯¹è¯
        ])
        
        # ä½¿ç”¨ç®€å•çš„ ConversationChainï¼Œä¸ä¾èµ–å¤æ‚çš„å†…å­˜ç®¡ç†
        conv = ConversationChain(
            llm=llm,
            verbose=chain_of_thought
        )
        
        # å°†ä¸Šä¸‹æ–‡å’Œå½“å‰é—®é¢˜ç»„åˆ
        if context:
            full_prompt = f"{context}\n\nHuman: {prompt}\nAssistant:"
        else:
            full_prompt = prompt
            
        result = conv.invoke({"input": full_prompt})
        
        print("[DEBUG] ConversationChain result:", result)

        if isinstance(result, dict):
            response = result.get("response") or list(result.values())[0]
        else:
            response = str(result)

    except Exception as e:
        response = f"âš ï¸ å¯¹è¯æ¨¡å¼å¼‚å¸¸ï¼š{str(e)}"

    # æ·»åŠ åˆ°å†…å­˜
    memory_manager.add_interaction(prompt, response)

    st.session_state.chat_history = st.session_state.get("chat_history", [])
    st.session_state.chat_history.append((prompt, response))

    return response